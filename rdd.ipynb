{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"C:\\Users\\DB400T2A\\spark-1.6.0-bin-hadoop2.6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x063C8170>\n"
     ]
    }
   ],
   "source": [
    "print sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'1.6.0'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'local[*]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc._conf.get(\"spark.jars.packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.app.name', u'myAppName'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.master', u'local[*]'),\n",
       " (u'spark.submit.deployMode', u'client')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=list([39.2,36.5,37.3,37.0])\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for x in c:\n",
    "        _c = 9./5*x+32\n",
    "        f.append(_c)\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 98.60000000000001]\n"
     ]
    }
   ],
   "source": [
    "print c2f(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 98.60000000000001]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def c2f_(c):\n",
    "    return 9./5*c+32\n",
    "\n",
    "map(c2f_,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 98.60000000000001]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5)*c+32,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈소스 클러스터 컴퓨팅 프래임워크이다.\n",
    "Originally developed at the University of California, Berkeley's AMPLab, \n",
    "the Spark codebase was later donated to the Apache Software Foundation, \n",
    "which has maintained it since. Spark provides an interface for programming entire clusters with \n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textFile=sc.textFile(\"data/ds_spark_wiki.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(textFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache Spark is an open source cluster computing framework.',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508\\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub798\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = textFile.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub798\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.'],\n",
       " [u'Originally',\n",
       "  u'developed',\n",
       "  u'at',\n",
       "  u'the',\n",
       "  u'University',\n",
       "  u'of',\n",
       "  u'California,',\n",
       "  u\"Berkeley's\",\n",
       "  u'AMPLab,',\n",
       "  u''],\n",
       " [u'the',\n",
       "  u'Spark',\n",
       "  u'codebase',\n",
       "  u'was',\n",
       "  u'later',\n",
       "  u'donated',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'Apache',\n",
       "  u'Software',\n",
       "  u'Foundation,',\n",
       "  u''],\n",
       " [u'which',\n",
       "  u'has',\n",
       "  u'maintained',\n",
       "  u'it',\n",
       "  u'since.',\n",
       "  u'Spark',\n",
       "  u'provides',\n",
       "  u'an',\n",
       "  u'interface',\n",
       "  u'for',\n",
       "  u'programming',\n",
       "  u'entire',\n",
       "  u'clusters',\n",
       "  u'with',\n",
       "  u''],\n",
       " [u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 31, 73, 72, 96, 46]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textFile\\\n",
    "    .map(lambda x:len(x))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_spark_line = textFile.filter(lambda line:u\"스파크\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spark_line.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈소스 클러스터 컴퓨팅 프래임워크이다.\n"
     ]
    }
   ],
   "source": [
    "print _spark_line.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myrdd = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squared = myrdd.map(lambda x:x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squared.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = [\"this is a line\",\"this is another line\"]\n",
    "myrdd2 = sc.parallelize(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = myrdd2.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'a', 'line'], ['this', 'is', 'another', 'line']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'a', 'line'], ['this', 'is', 'another', 'line']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd2.map(lambda x:x.split(' ')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is AA line', 'this is AAnother line']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myrdd2.map(lambda x:x.replace(\"a\",\"AA\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp_file = sc.textFile(\"./data/ds_spark_2cols.csv\")\n",
    "numbers_rdd = inp_file.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1'],\n",
       " [u'14', u' 19'],\n",
       " [u'46', u' 1'],\n",
       " [u'10', u' 34'],\n",
       " [u'28', u' 3'],\n",
       " [u'48', u' 1']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "dv1 = Vectors.dense([0.0, 1.1, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0,1.1,0.1]\n"
     ]
    }
   ],
   "source": [
    "print dv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dv2 = [1.0, 2.1, 3.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.]\n"
     ]
    }
   ],
   "source": [
    "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\n",
    "print sv1.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.0,3.0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "LabeledPoint(1.0, Vectors.dense([1.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [[1,[1.0,2.0,3.0]],[1,[1.1,2.1,3.1]],[0,[1.2,2.2,3.3]]]\n",
    "trainDf=sqlCtx.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = sqlCtx.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "_rdd = sc.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf=_rdd.toDF(schema)\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svmfn=\"C:\\Users\\DB400T2A\\spark-1.6.0-bin-hadoop2.6\\data\\mllib\\sample_libsvm_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DB400T2A\\spark-1.6.0-bin-hadoop2.6\\data\\mllib\\sample_libsvm_data.txt\n"
     ]
    }
   ],
   "source": [
    "print svmfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svmDf=sqlCtx.read.format(\"libsvm\").load(svmfn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svmDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svmDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-3: RDD를 사용하여 MLlib의 입력 데이터 word vector생성하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"data/ds_spark_wiki.txt\")\n",
    "wc = lines\\\n",
    "    .flatMap(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache',\n",
       " u'Spark',\n",
       " u'is',\n",
       " u'an',\n",
       " u'open',\n",
       " u'source',\n",
       " u'cluster',\n",
       " u'computing',\n",
       " u'framework.',\n",
       " u'\\uc544\\ud30c\\uce58',\n",
       " u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       " u'\\uc624\\ud508\\uc18c\\uc2a4',\n",
       " u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       " u'\\ucef4\\ud4e8\\ud305',\n",
       " u'\\ud504\\ub798\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Originally',\n",
       " u'developed',\n",
       " u'at',\n",
       " u'the',\n",
       " u'University',\n",
       " u'of',\n",
       " u'California,',\n",
       " u\"Berkeley's\",\n",
       " u'AMPLab,',\n",
       " u'',\n",
       " u'the',\n",
       " u'Spark',\n",
       " u'codebase',\n",
       " u'was',\n",
       " u'later',\n",
       " u'donated',\n",
       " u'to',\n",
       " u'the',\n",
       " u'Apache',\n",
       " u'Software',\n",
       " u'Foundation,',\n",
       " u'',\n",
       " u'which',\n",
       " u'has',\n",
       " u'maintained',\n",
       " u'it',\n",
       " u'since.',\n",
       " u'Spark',\n",
       " u'provides',\n",
       " u'an',\n",
       " u'interface',\n",
       " u'for',\n",
       " u'programming',\n",
       " u'entire',\n",
       " u'clusters',\n",
       " u'with',\n",
       " u'',\n",
       " u'implicit',\n",
       " u'data',\n",
       " u'parallelism',\n",
       " u'and',\n",
       " u'fault-tolerance.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wc = sc.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n",
    "    .map(lambda x:x.split())\\\n",
    "    .map(lambda x:[(i,1) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'wikipedia', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'is', 1), (u'an', 1), (u'open', 1), (u'source', 1), (u'cluster', 1), (u'computing', 1), (u'framework', 1)]\n",
      "[(u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c\\ub294', 1), (u'\\uc624\\ud508\\uc18c\\uc2a4', 1), (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1), (u'\\ucef4\\ud4e8\\ud305', 1), (u'\\ud504\\ub798\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4', 1)]\n",
      "[(u'originally', 1), (u'developed', 1), (u'at', 1), (u'the', 1), (u'university', 1), (u'of', 1), (u'california', 1), (u\"berkeley's\", 1), (u'amplab', 1)]\n",
      "[(u'the', 1), (u'spark', 1), (u'codebase', 1), (u'was', 1), (u'later', 1), (u'donated', 1), (u'to', 1), (u'the', 1), (u'apache', 1), (u'software', 1), (u'foundation', 1)]\n",
      "[(u'which', 1), (u'has', 1), (u'maintained', 1), (u'it', 1), (u'since', 1), (u'spark', 1), (u'provides', 1), (u'an', 1), (u'interface', 1), (u'for', 1), (u'programming', 1), (u'entire', 1), (u'clusters', 1), (u'with', 1)]\n",
      "[(u'implicit', 1), (u'data', 1), (u'parallelism', 1), (u'and', 1), (u'fault', 1), (u'tolerance', 1)]\n"
     ]
    }
   ],
   "source": [
    "for e in wc.collect():\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pDF= sqlCtx.read.json(\"C:\\Users\\DB400T2A\\spark-1.6.0-bin-hadoop2.6/examples/src/main/resources/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pDF.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlCtx.sql(\"select name from people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twitterDF = sqlCtx.read.json(\"src/ds_twitter_1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitterDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DB400T2A\\spark-1.6.0-bin-hadoop2.6\\python\\pyspark\\sql\\context.py:237: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    }
   ],
   "source": [
    "wcDf=sqlCtx.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcDf.registerTempTable(\"wc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club AtlÃ©tico Ta...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlCtx.sql(\"select Club,Team,Year from wc\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sqlCtx.createDataFrame(\n",
    "    [\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'false', 'good'],\n",
    "        ['Yes','young', 'true', 'true', 'fair'],\n",
    "        ['No','young', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'fair'],\n",
    "        ['No','middle', 'false', 'false', 'good'],\n",
    "        ['Yes','middle', 'true', 'true', 'good'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','middle', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'excellent'],\n",
    "        ['Yes','old', 'false', 'true', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'good'],\n",
    "        ['Yes','old', 'true', 'false', 'excellent'],\n",
    "        ['No','old', 'false', 'false', 'fair'],\n",
    "    ],\n",
    "    ['cls','age','f1','f2','f3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"cls\", outputCol=\"labels\")\n",
    "model=labelIndexer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1=model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cls: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- f1: string (nullable = true)\n",
      " |-- f2: string (nullable = true)\n",
      " |-- f3: string (nullable = true)\n",
      " |-- labels: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----+---------+------+\n",
      "|cls|   age|   f1|   f2|       f3|labels|\n",
      "+---+------+-----+-----+---------+------+\n",
      "| No| young|false|false|     fair|   1.0|\n",
      "| No| young|false|false|     good|   1.0|\n",
      "|Yes| young| true|false|     good|   0.0|\n",
      "|Yes| young| true| true|     fair|   0.0|\n",
      "| No| young|false|false|     fair|   1.0|\n",
      "| No|middle|false|false|     fair|   1.0|\n",
      "| No|middle|false|false|     good|   1.0|\n",
      "|Yes|middle| true| true|     good|   0.0|\n",
      "|Yes|middle|false| true|excellent|   0.0|\n",
      "|Yes|middle|false| true|excellent|   0.0|\n",
      "|Yes|   old|false| true|excellent|   0.0|\n",
      "|Yes|   old|false| true|     good|   0.0|\n",
      "|Yes|   old| true|false|     good|   0.0|\n",
      "|Yes|   old| true|false|excellent|   0.0|\n",
      "| No|   old|false|false|     fair|   1.0|\n",
      "+---+------+-----+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"age\", outputCol=\"att1\")\n",
    "model=labelIndexer.fit(df1)\n",
    "df2=model.transform(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"f1\", outputCol=\"att2\")\n",
    "model=labelIndexer.fit(df2)\n",
    "df3=model.transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"f2\", outputCol=\"att3\")\n",
    "model=labelIndexer.fit(df3)\n",
    "df4=model.transform(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol=\"f3\", outputCol=\"att4\")\n",
    "model=labelIndexer.fit(df4)\n",
    "df5=model.transform(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-----+---------+------+----+----+----+----+\n",
      "|cls|   age|   f1|   f2|       f3|labels|att1|att2|att3|att4|\n",
      "+---+------+-----+-----+---------+------+----+----+----+----+\n",
      "| No| young|false|false|     fair|   1.0| 0.0| 0.0| 0.0| 1.0|\n",
      "| No| young|false|false|     good|   1.0| 0.0| 0.0| 0.0| 0.0|\n",
      "|Yes| young| true|false|     good|   0.0| 0.0| 1.0| 0.0| 0.0|\n",
      "|Yes| young| true| true|     fair|   0.0| 0.0| 1.0| 1.0| 1.0|\n",
      "| No| young|false|false|     fair|   1.0| 0.0| 0.0| 0.0| 1.0|\n",
      "| No|middle|false|false|     fair|   1.0| 1.0| 0.0| 0.0| 1.0|\n",
      "| No|middle|false|false|     good|   1.0| 1.0| 0.0| 0.0| 0.0|\n",
      "|Yes|middle| true| true|     good|   0.0| 1.0| 1.0| 1.0| 0.0|\n",
      "|Yes|middle|false| true|excellent|   0.0| 1.0| 0.0| 1.0| 2.0|\n",
      "|Yes|middle|false| true|excellent|   0.0| 1.0| 0.0| 1.0| 2.0|\n",
      "|Yes|   old|false| true|excellent|   0.0| 2.0| 0.0| 1.0| 2.0|\n",
      "|Yes|   old|false| true|     good|   0.0| 2.0| 0.0| 1.0| 0.0|\n",
      "|Yes|   old| true|false|     good|   0.0| 2.0| 1.0| 0.0| 0.0|\n",
      "|Yes|   old| true|false|excellent|   0.0| 2.0| 1.0| 0.0| 2.0|\n",
      "| No|   old|false|false|     fair|   1.0| 2.0| 0.0| 0.0| 1.0|\n",
      "+---+------+-----+-----+---------+------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
